
 
'''
# Ã–rnek 1: llm nasÄ±l Ã§alÄ±ÅŸÄ±r?

from openai import OpenAI
import json
from dotenv import load_dotenv

load_dotenv()

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "sen bir bilgi asistanisin."},
        {"role": "user", "content": "TÃ¼rkiyenin baÅŸkenti neresi, json formatinda yaz."},

    ],
    response_format={"type": "json_object"}
)
data = json.loads(response.choices[0].message.content)
print(data)
print(data.get("cevap"))
print(data.get("kaynak", "belirtilmemiÅŸ"))

'''


 
'''
# Ã–rnek 2 : llm orkestrasyonu
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, Tool
from dotenv import load_dotenv
load_dotenv()

llm = ChatOpenAI(model="gpt-4o")

def toplama(x,y):
    return int(x) + int(y)

tools = [
    Tool(
        name="toplama araci",
        func=lambda x: toplama(*[s.strip(" ()") for s in x.split(",")]),
        description="iki sayiyi toplayan araÃ§tÄ±r."
    )
]
agent = initialize_agent(tools, llm, agent_type="zero-shot-react-description")
print(agent.run("12 ile 8'i topla ve sonucu bana anlat"))
'''
# Ã–rnek 3 : langchain
'''
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
load_dotenv()

llm = ChatOpenAI(model="gpt-4o")

prompt = ChatPromptTemplate.from_template("Soru: {soru}")

sonuc = llm.invoke(prompt.format(soru="TÃ¼rkiye'nin en bÃ¼yÃ¼k yÃ¼z Ã¶lÃ§Ã¼mÃ¼ne sahip ÅŸehri hangisidir?"))

print("model cevabÄ±:", sonuc.content)

'''

# Ã–rnek 4 : langchain Ã§eviri
'''
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain, SimpleSequentialChain
from dotenv import load_dotenv

load_dotenv()

llm = ChatOpenAI(model="gpt-4o")

translate_prompt = ChatPromptTemplate.from_template("Ã‡evir (TÃ¼rkÃ§e -> ingilizce): {soru}")
translate_chain = LLMChain(llm=llm, prompt=translate_prompt)

answer_prompt = ChatPromptTemplate.from_template("Question: {text}")
answer_chain = LLMChain(llm=llm, prompt=answer_prompt)

overall_chain = SimpleSequentialChain(chains=[translate_chain, answer_chain])

soru = "DÃ¼nyanÄ±n en yÃ¼ksek daÄŸÄ± hangsidir?"
print("sonuÃ§:", overall_chain.run(soru))
'''

# Ã–rnek 5 : mesajÄ± hafÄ±zaya kaydetmek
'''
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from dotenv import load_dotenv

load_dotenv()

llm =  ChatOpenAI(model="gpt-4o")

memory = ConversationBufferMemory()

conversation = ConversationChain(llm=llm, memory=memory)

print(conversation.run("merhaba ben ilknur"))
print(conversation.run("beni hatÄ±rlÄ±yor musun?"))
'''

# Ã–rnek 6: json formatÄ±nda cvb alma
'''
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.output_parsers import ResponseSchema, StructuredOutputParser
from dotenv import load_dotenv

load_dotenv()

llm = ChatOpenAI(model="gpt-4o")
schemas = [
    ResponseSchema(name="cevap", description="sorunun cevabÄ±"),
    ResponseSchema(name="kaynak", description="bilgi kaynaÄŸÄ±")
]
parser = StructuredOutputParser.from_response_schemas(schemas)
format_instr = parser.get_format_instructions()

prompt = ChatPromptTemplate.from_template(
    "Soru: {soru}\nCevabÄ± ÅŸu formatta ver: {format_instr}"
)
chain = LLMChain(llm=llm, prompt=prompt)

result= chain.run({"soru": "TÃ¼rkiye'nin baÅŸeknti neredir?", "format_instr": format_instr})
print(parser.parse(result))
'''

# Ã–rnek 7 : prompt
'''

from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

llm = ChatOpenAI(model="gpt-4o")

prompt = ChatPromptTemplate.from_template("kullanÄ±cÄ±: {soru}\nAsistan:")

soru1 = llm.invoke(prompt.format(soru="dÃ¼naynÄ±n en bÃ¼yÃ¼k okyanusu hangisidir"))
soru2 = llm.invoke(prompt.format(soru="pyhton da llm yazma kurallarÄ± nedir?"))

print("cevap1:", soru1.content)
print("cevap2:", soru2.content)

'''



# Ã–rnek 8 : langserve
'''
from fastapi import FastAPI
from langserve import add_routes
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()
app = FastAPI()

llm = ChatOpenAI(model="gpt-4o")
prompt = ChatPromptTemplate.from_template("Soru: {soru}")
chain = prompt | llm

add_routes(app, chain, path="/chat")
'''

# Ã–rnek 9 : langchain
'''
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import os 

load_dotenv()

os.environ["LANGCHAIN_TRACÄ°NG_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "lc-123456"

llm = ChatOpenAI(model="gpt-4o")
prompt = ChatPromptTemplate.from_template("Soru: {soru}")
chain = prompt | llm

print(chain.invoke("LLM orkestrasyonu nedir?"))
'''

# Ã–rnek 10 : mesajlaÅŸma tarihi
'''
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain 
from datetime import datetime
from dotenv import load_dotenv
 
load_dotenv()

llm = ChatOpenAI(model="gpt-4o")

prompt = ChatPromptTemplate.from_template(
    "Tarih: {tarih}\nKullanÄ±cÄ±: {soru}\nAsistan:"
)
chain = prompt | llm

tarih = datetime.now().strftime("%y-%m-%d %H:%M:%S")

cevap = chain.invoke({"tarih": tarih, "soru": "BugÃ¼n hava nasÄ±l?"})
print(cevap)

'''

# Ã–rnek 11 : mesaj geÃ§miÅŸi
'''
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from dotenv import load_dotenv

load_dotenv()

llm = ChatOpenAI(model="gpt-4o")

memory = ConversationBufferMemory(return_messages=True)

conversation = ConversationChain(llm=llm, memory=memory)

conversation.run("benim adÄ±m ilknur")
conversation.run("doÄŸum yÄ±lÄ±mÄ± ve nerde doÄŸduÄŸumu hatÄ±rlÄ±yormusun?")
print("\n----KonuÅŸma GeÃ§miÅŸi----")

for msg in memory.chat_memory.messages:
    print(f"{msg.type.upper()}: {msg.content}")

    '''


# Ã–rnek 12 : streaming ve memory
'''
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain 
from langchain.callbacks.base import BaseCallbackHandler
from dotenv import load_dotenv

load_dotenv()
class StreamHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(token, end="", flush=True)

llm = ChatOpenAI(model="gpt-4o", streaming=True, callbacks=[StreamHandler()])
memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory)
print("\n----Streaming Cevap----")
conversation.run("yapay zekann geleceÄŸini anlat")
'''

# Ã–rnek 13 : vektÃ¶rstore
'''
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

load_dotenv()

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
texts = [
    "Ankara TÃ¼rkiyenin baÅŸkentidir.",
    "Ä°stanbul TÃ¼rkiye'nin en bÃ¼yÃ¼k ÅŸeridir.",
    "Ä°zmir ege bÃ¶lgesinde bir liman kentidir.",    
]
vectorstore = FAISS.from_texts(texts, embeddings)
print("vector sayÄ±sÄ±:", vectorstore.index.ntotal)

'''

# Ã–rnek 14 : rag
'''
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv


load_dotenv()

llm = ChatOpenAI(model="gpt-4o")

retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 2})
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff" #belgeleri topla + cevapla
)

cevap = qa_chain.run("TÃ¼ekiyenin en kalabalÄ±k ÅŸehri hangisidir?")
print("cevap:", cevap)
'''

# Ã–rnek 15 : internetten veri Ã§ekmek
'''
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, Tool
from dotenv import load_dotenv

load_dotenv()

llm = ChatOpenAI(model="gpt-4o", temperature=0)

search = DuckDuckGoSearchRun()
tools = [
    Tool(
        name="web arama",
        func=search.run,
        description="internetten veri aramak iÃ§in kullanÄ±lÄ±r"
    )
]
agent = initialize_agent(tools, llm, agent_type="zero-shot-react-description", verbose=True)
cevap = agent.run("TÃ¼rkiyenin 2023 nÃ¼fusu nedir?")
print("cevap:", cevap)
'''

# Ã–rnek 16 : vektÃ¶rize ve text bÃ¶lme
'''
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

load_dotenv()
text = """
yapay zeka modelelri, bÃ¼yÃ¼k veri setleri Ã¼zerinde eÄŸitilir.
transformer mimarisi, dikkat (attention) mekanizmasÄ± ile Ã§alÄ±ÅŸÄ±r.
LLM ler insan dilini anlamak ve Ã¼retmek iÃ§in  kullanÄ±lÄ±r.
"""
splitter = RecursiveCharacterTextSplitter(
    chunk_size=50,
    chunk_overlap=10
)
docs = splitter.split_text(text)
print("parÃ§alar:", docs)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = FAISS.from_texts(docs, embeddings)
print("toplam vektÃ¶r:", vectorstore.index.ntotal)
'''
# Ã–rnek 17 : hub
'''
from langchain import hub
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

prompt = hub.pull("rlm/rag-prompt")#Ã¶zetleme yapar
llm = ChatOpenAI(model="gpt-4o")

chain = prompt | llm
cevap = chain.invoke({ 
    "context": "langchain LLM'lerle Ã§alÄ±ÅŸma sÃ¼reci hakkÄ±nda bilgiler.",
    "question": "Bu sÃ¼recci nasÄ±l kolaylaÅŸtÄ±r?"
})
print("Ã¶zet:", cevap.content)
'''

# Ã–rnek 18 : streaming
'''
from langchain_openai import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
from dotenv import load_dotenv
load_dotenv()
class StreamHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(token, end="", flush=True)

 
llm = ChatOpenAI(
    model="gpt-4o",
    streaming=True,
    callbacks=[StreamHandler()],
    temperature=0.7

)
llm.invoke("yapay zekanÄ±n gelecekteki en bÃ¼yÃ¼k etkilerini aÃ§Ä±kla")
print("\n\nTam cevap:", cevap.content)
'''


# Ã–rnek 19 : rag
'''
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv

load_dotenv()

docs = [
    "ankara TÃ¼rkiye'nin baÅŸkrntidir.",
    "istanbul tÃ¼rkiyenin en kalabalÄ±k ÅŸehridir.",
    "everest dÃ¼nyanÄ±n en yÃ¼ksek daÄŸÄ±dÄ±r"
]

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = FAISS.from_texts(docs, embeddings)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 1})
llm = ChatOpenAI(model="gpt-4o")
assistant = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff")

print("Soru:", "TÃ¼rkiyenin baÅŸkenti neresidir")
print("asistan:", assistant.run("tÃ¼rkiyenin baÅŸkenti neresi?"))
'''

# Ã–rnek 20 : agent
'''
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, Tool
from langchain_community.tools import DuckDuckGoSearchRun
from dotenv import load_dotenv
load_dotenv()
llm = ChatOpenAI(model="gpt-4o")

def hesapla(expression):
    return eval(expression)

tools = [
    Tool(name="Hesaplama", func=hesapla, description="matematik iÅŸlemleri yapar"),
    Tool(name="web arama", func=DuckDuckGoSearchRun().run, description="internetten blgi alÄ±r")
]
agent = initialize_agent(tools, llm, agent_type="zero-shot-react-destription", verbose=True)
print(agent.run("12 * 8 kaÃ§ eder?"))
print(agent.run("TÃ¼rkiye de en Ã§ok kullanÄ±lan kÄ±z isimleri?"))
'''

# Ã–rnek 21 : agent entegrasyonu

'''
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, Tool
from dotenv import load_dotenv
import requests
import os

load_dotenv()
taviy_key = os.getenv("TAVILY_API_KEY")

llm = ChatOpenAI(model="gpt-4o")

def tavily_search(query: str):
    url = "https://api.tavily.com/search"
    headers = {"Authorization": "Bearer tv_XXXXXXX"}
    resp = requests.post(url, headers=headers, json={"q": query})
    return resp.json()

def hesapla(exp):
    return eval(exp)
tools = [
    Tool(name="hesaplama", func=hesapla, description="matematik iÅŸlemleri yapar"),
    Tool(name="Tavily arama", func=tavily_search, description="Tavily ile internetten arama yapar.")
]
agent = initialize_agent(tools, llm, agent_type="zero-shot-react-description", verbose=True)
print(agent.run("200 * 10 kaÃ§ eder?"))
print(agent.run("BugÃ¼n TÃ¼rkiye'nin enflasyon oranÄ± nedir"))

'''

# Ã–rnek 22 : agent memory ####
'''
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, Tool, AgentType
from langchain.memory import ConversationBufferMemory
from dotenv import load_dotenv

load_dotenv()

llm = ChatOpenAI(model="gpt-4o")

def hesapla(expression):
    return eval(expression)

tools = [
    Tool(name="hesaplama", func=hesapla, description="matematiksel iÅŸlemler yapar.")

]
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent_type=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
    memory=memory,
    verbose=True
)
print(agent.run("merhaba ben ilknur"))
print(agent.run("12*8 kaÃ§ eder"))
print(agent.run("beni hatÄ±rlÄ±yor musun"))
'''
# Ã–rnek 23 : retrieval notlandÄ±rÄ±cÄ±sÄ±
'''
from langchain.evaluation import load_evaluator
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()
llm = ChatOpenAI(model="gpt-4o")

retrieval_eval = load_evaluator("context_qa", llm=llm)
expected="Ankara"
predicted_docs = ["istanbul tÃ¼rkiyenin en bÃ¼yÃ¼k ÅŸehridirÃ§.", "Ankara TÃ¼rkiye'nin baÅŸkentidir"]
result = retrieval_eval.evaluate_strings(
    prediction=";".join(predicted_docs),
    reference=expected, 
    input="TÃ¼rkiye'nin baÅŸkenti neresidr"

)
print("SonuÃ§:",result)
'''

# Ã–rnek 24 : halisinasyon notlandÄ±rÄ±cÄ±sÄ±
'''
from langchain_openai import ChatOpenAI
from langchain.evaluation import load_evaluator
from dotenv import load_dotenv

load_dotenv()
llm = ChatOpenAI(model="gpt-4o")
custom_criteria = {
    "hallucination": "Cevap, verilen baÄŸlamda yer almayan veya yanlÄ±ÅŸ bilgilerle uyduruyor mu?"
}

hallucination_eval = load_evaluator(
    "criteria",
    criteria=custom_criteria,
    llm=llm
)

context = "ankara tÃ¼rkiyenin baÅŸkentidir."
answer = "TÃ¼rkiye'nin baÅŸkenti Ä°stanbul'dur."
result = hallucination_eval.evaluate_strings(
    prediction=answer,
    input="TÃ¼rkiye'nin baÅŸkenti neresidir?",
    reference=context
)

print("sonuÃ§:", result)
'''


# Ã–rnek 25 : cevap notlandÄ±rÄ±cÄ±sÄ±
'''
from langchain_openai import ChatOpenAI
from langchain.evaluation import load_evaluator
from dotenv import load_dotenv
load_dotenv()

llm = ChatOpenAI(model="gpt-4o")

answer_eval = load_evaluator("qa", llm=llm)

question = "TÃ¼rkiye'nin en az nÃ¼fÃ¼sa sahip ili neresidir?"
answer = "TÃ¼rkiye'nin en az nÃ¼fusa sahip ili Bayburt'dur."
result = answer_eval.evaluate_strings(
    prediction=answer,
    input=question,
    reference="Bayburt"

)
print("deÄŸerlendirme:", result)
'''
# Ã–rnek 26 : agent tool kullanÄ±mÄ±
'''
from langchain.agents import initialize_agent, Tool, AgentType
from langchain_experimental.tools import PythonREPLTool
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

llm = ChatOpenAI(model="gpt-4o")

tools = [PythonREPLTool()]

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent_type=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
    verbose=True
)

print(agent.run("25'in karesini hesapla"))
'''

# Ã–rnek 27 : retrieval
'''
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from dotenv import load_dotenv

load_dotenv()

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

docs = [
  "LLM diÄŸer gÃ¶revlerin aynÄ± sÄ±ra metin tanÄ±yÄ±p Ã¼retbilen bir tÃ¼r AI programÄ±dÄ±r",
  "LLM' ler bÃ¼yÃ¼k veri kÃ¼mleri Ã¼zerinde eÄŸitilir.",
  "LLM'ler makina Ã§ÄŸrenmesi Ã¼zerinde kuruludur."  

]
db = FAISS.from_texts(docs, embeddings)
retriever = db.as_retriever()
llm = ChatOpenAI(model="gpt-4o")
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

print(qa.run("LLM'ler de hangi Ã¶ÄŸrenme yÃ¶ntemi uygulnÄ±r?"))
'''

# Ã–rnek 28 : cevap kontrol
'''
from langchain.evaluation import load_evaluator
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
load_dotenv()

llm = ChatOpenAI(model="gpt-4o")

criteria = {
    "correctness": "cevap doÄŸru bilgi iÃ§eriyormu"
}
evaluator = load_evaluator("criteria", criteria=criteria, llm=llm)

context = "penguenler kutup bÃ¶legelerinde yaÅŸarlar"
answer = "penguenler Ã§Ã¶lde yaÅŸarlar."
result = evaluator.invoke({
    "input": "Penguenler hangi bÃ¶lgelerde yaÅŸar",
    "output": answer,
    "reference": context
})

print(result)
'''
'''
from cleverbotfree import Cleverbot

@Cleverbot.connect
def chat(bot, user_prompt, bot_prompt):
    while True:
        user_input = input(user_prompt)
        if user_input == "quit":
            break
        reply = bot.single_helloexchange(user_input)#text ister
        print(bot_prompt, reply)
    bot.close()

chat("You:", "Cleverbot:")
'''

# Ã–rnek 29 :
'''
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.agents import initialize_agent, AgentType
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
 

load_dotenv()

llm = ChatOpenAI(model="gpt-4o", temperature=0)

search = TavilySearchResults(max_results=2)
tools = [search]
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True

)
if __name__ == "__main__":
    print("ðŸ¤–chatbota hosÅŸgeldiniz Ã§Ä±kmak iÃ§in 'exit' yazÄ±n.")
    while True:
        user_input = input("> ")
        if user_input.lower() in ["exit", "quit"]:
            break
        response = agent.run(user_input)
        print(response)
        '''
'''

# Ã–rnek 30 : Tavily ile arama
from langchain_tavily import TavilySearch
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor
from langgraph.prebuilt import create_react_agent
from langchain.memory import ConversationSummaryMemory
from dotenv import load_dotenv 

load_dotenv()

model = ChatOpenAI(model="gpt-4o", temperature=0)

search = TavilySearch(max_results=2)
tools = [search]

# LangGraph'ta sadece model ve tools gerekiyor
agent = create_react_agent(
    model,
    tools
)

config = {"configurable": {"thread_id": "abc123"}}

if __name__ == "__main__":
    print("ðŸ¤– chat baÅŸlatÄ±ldÄ±. Ã‡Ä±kmak iÃ§in 'exit' e basÄ±n")
    while True:
        user_input = input("> ")
        if user_input.lower() in ["exit", "quit"]:
            print("baÅŸarÄ±yla Ã§Ä±kÄ±ÅŸ yapÄ±ldÄ±...")
            break

        response = agent.invoke(
            {"input": user_input},
            config=config
        )
        print(response)


'''
# Ã–rnek 31 : Flask API
'''
from flask import Flask, request, jsonify
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

app = Flask(__name__)
client = OpenAI()

 
def add_numbers(a, b):
    return a + b

def multiply_numbers(a, b):
    return a * b

custom_functions = {
    "add": add_numbers,
    "multiply": multiply_numbers
}

@app.route("/start", methods=["GET", "POST"])
def chat():
    
    if request.method == "GET":
        return "Start endpoint GET isteÄŸi Ã§alÄ±ÅŸtÄ±!"

    
    if request.method == "POST":
        try:
            data = request.get_json()
            user_message = data.get("message", "")
            function_call = data.get("function", None)


           
            if function_call and function_call in custom_functions:
                args = data.get("args", {})
                result = custom_functions[function_call](**args)
                return jsonify({"result": result})

             
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Sen bir yardÄ±mcÄ± asistansÄ±n"},
                    {"role": "user", "content": user_message}
                ]
            )
            return jsonify({"response": response.choices[0].message.content})
            
        except Exception as e:
            return jsonify({"error": str(e)})

if __name__ == "__main__":
    app.run(debug=True)
'''

### Ã–rnek 32 : Flask, Bellek, Tool
'''
from flask import Flask, request, jsonify
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, Tool
from langchain.memory import ConversationBufferMemory
from langchain_community.tools import DuckDuckGoSearchRun
from dotenv import load_dotenv

load_dotenv()
app = Flask(__name__)

llm = ChatOpenAI(model="gpt-4o", temperature=0)
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

def hesapla(expression: str) -> str:
    try:
        return str(eval(expression))
    except Exception as e:
        return f"Hata: {str(e)}"

calculator = Tool(
    name="calculator",
    func=hesapla,
    description="Matematiksel ifadeleri hesaplar."
)
search = DuckDuckGoSearchRun()
tools = [calculator, search]

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent="chat-conversational-react-description",
    verbose=True,
    memory=memory
)

@app.route("/chat", methods=["POST"])
def chat():
    try:
        data = request.get_json()
        user_input = data.get("message")
        result = agent.run(user_input)
        return jsonify({"response": result})
    except Exception as e:
        return jsonify({"error": str(e)})

if __name__ == "__main__":
    app.run(debug=True)
    '''
'''
# Ã–rnek 33 : SQLite
from langchain_openai import ChatOpenAI
from langchain_tavily import TavilySearch
from langgraph.prebuilt import create_react_agent
from dotenv import load_dotenv

 
load_dotenv()

 
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)


search = TavilySearch(max_results=2)
tools = [search]


agent = create_react_agent(llm, tools)


config = {"configurable": {"thread_id": "test123"}}


if __name__ == "__main__":
    print("ðŸ¤– Agent test ediliyor...\n")
    
    
    response1 = agent.invoke(
        {"messages": [("user", "Merhaba! Sen kimsin?")]}, 
        config=config
    )
    print("Cevap 1:", response1["messages"][-1].content)
    print("-" * 50)
    
    
    response2 = agent.invoke(
        {"messages": [("user", "DÃ¼n bana ne demiÅŸtin?")]}, 
        config=config
    )
    print("Cevap 2:", response2["messages"][-1].content)
    print("-" * 50)
    
    
    response3 = agent.invoke(
        {"messages": [("user", "TÃ¼rkiye'nin baÅŸkenti nedir?")]}, 
        config=config
    )
    print("Cevap 3:", response3["messages"][-1].content)

    '''
'''
# Ã–rnek 34 : SQLite
from dotenv import load_dotenv
import sqlite3

load_dotenv()
conn = sqlite3.connect("okul.db")
cursor = conn.cursor()
cursor.execute("""
CREATE TABLE IF NOT EXISTS ogrenciler(
    id INTEGER PRIMARY KEY AUTOINCREMENT,  --Her Ã¶ÄŸrenciye otomatik ID ver
    ad TEXT ,                              -- Ã–ÄŸrencinin adÄ±
    soyad TEXT ,                           -- Ã–ÄŸrencinin soyadÄ±
    notu INTEGER                           -- Ã–ÄŸrencinin notu
)
""")
def ogrenci_ekle(ad, soyad, notu):
    cursor.execute("INSERT INTO ogrenciler (ad, soyad, notu) VALUES (?, ?, ?)", (ad, soyad, notu))
    conn.commit()

def ogrencileri_listele():
    cursor.execute("SELECT * FROM ogrenciler")
    ogrenciler = cursor.fetchall()
    for ogrenci in ogrenciler:
        print(ogrenci)

def notlarÄ±_guncelle(ogrenci_id, yeni_not):
    cursor.execute("UPDATE ogrenciler SET notu = ? WHERE id = ?", (yeni_not, ogrenci_id))
    conn.commit()

def notlarÄ±_sil(ogrenci_id):
    cursor.execute("DELETE FROM ogrenciler WHERE id = ?", (ogrenci_id,))
    conn.commit()

def listele():
    return ogrenciler

ogrenci_ekle("Ali", "YÄ±lmaz", 85)
ogrenci_ekle("AyÅŸe", "Kara", 90)
ogrenci_ekle("Mehmet", "Demir", 78)

print("Ã–ÄŸrenciler:", ogrencileri_listele())

notlarÄ±_guncelle(1, 95)
print("GÃ¼ncel Liste:", ogrencileri_listele())

ogrenci_ekle("Fatma", "Ã‡elik", 88)
print("GÃ¼ncellenmiÅŸ Ã–ÄŸrenciler:", ogrencileri_listele())

'''


# Ã–rnek 35 : Flask API
'''
from flask import Flask, request, jsonify
import sqlite3
from dotenv import load_dotenv
load_dotenv()

app = Flask(__name__)

@app.route("/")
def home():
    return "hoÅŸgeldiniz flask sistemi Ã§alÄ±ÅŸÄ±yor"


def get_db():
    conn = sqlite3.connect("okul.db")
    conn.row_factory = sqlite3.Row
    return conn

@app.route("/ogrenciler", methods=["GET"])
def ogrencileri_listele_api():
    conn = get_db()
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM ogrenciler")
    ogrenciler = [dict(row) for row in cursor.fetchall()]
    return jsonify(ogrenciler)

@app.route("/ekle", methods=["POST"])
def ekle():
    data = request.json
    conn = get_db()
    cursor = conn.cursor()
    cursor.execute("INSERT INTO ogrenciler (ad, soyad, notu) VALUES (?, ?, ?)",
                   (data["ad"], data["soyad"], data["notu"]))
    conn.commit()
    return jsonify({"message": "Ã–ÄŸrenci eklendi."})

@app.route("/guncelle/<int:id>", methods=["PUT"])
def guncelle(id):
    data = request.json
    conn = get_db()
    cursor = conn.cursor()
    cursor.execute("UPDATE ogrenciler SET notu = ? WHERE id = ?", (data["notu"])),
    conn.commit()
    return jsonify({"mesaj": "not guncellendi"})

@app.route("/sil/<int:id>", methods=["DELETE"])
def sil(id):
    conn = get_db()
    cursor = conn.cursor()
    cursor.execute("DELETE FROM ogrenciler WHERE id = ?", (id,))
    conn.commit()
    return jsonify({"mesaj": "Ã¶ÄŸrenci silindi"})

if __name__ == "__main__":
    app.run(debug=True)
'''
